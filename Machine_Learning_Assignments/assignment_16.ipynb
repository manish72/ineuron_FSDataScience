{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {},
   "source": [
    "##### 1. In a linear equation, what is the difference between a dependent variable and an independent variable ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50379b27",
   "metadata": {},
   "source": [
    "**Ans:** Algebraically, a linear equation typically takes the form y = mx + b, where m and b are constants, x is the independent variable, y is the dependent variable. The slope tells us how the dependent variable (y) changes for every one unit increase in the independent (x) variable, on average.\n",
    "\n",
    "The variables in a study of a cause-and-effect relationship are called the independent and dependent variables. The independent variable is the cause. Its value is independent of other variables in your study. The dependent variable is the effect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {},
   "source": [
    "##### 2. What is the concept of simple linear regression? Give a specific example ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "830c3c42",
   "metadata": {},
   "source": [
    "**Ans:** Simple linear regression is a regression model that estimates the relationship between one independent variable and one dependent variable using a straight line. Both variables should be quantitative. Linear regression most often uses mean-square error (MSE) to calculate the error of the model.\n",
    "\n",
    "For example, suppose that height was the only determinant of body weight.In this example, if an individual was 70  inches tall, we would predict his weight to be: `Weight = 80 + 2 x (70) = 220 lbs`. In this simple linear regression, we are examining the impact of one independent variable on the outcome."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {},
   "source": [
    "##### 3. In a linear regression, define the slope ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12b56171",
   "metadata": {},
   "source": [
    "**Ans:** The slope of a regression line (b) represents the rate of change in y as x changes. Because y is dependent on x, the slope describes the predicted values of y given x. The slope must be calculated before the y-intercept when using a linear regression, as the intercept is calculated using the slope."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {},
   "source": [
    "##### 4. Determine the graph's slope, where the lower point on the line is represented as (3, 2) and the higher point is represented as (2, 2) ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a9ffb7c",
   "metadata": {},
   "source": [
    "**Ans:** The graph's slope, where the lower point on the line is represented as (3, 2) and the higher point is represented as (2, 2) is 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {},
   "source": [
    "##### 5. In linear regression, what are the conditions for a positive slope ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2bc0488",
   "metadata": {},
   "source": [
    "**Ans:** If the slope is positive, y increases as x increases, and the function runs \"uphill\" (going left to right). If the slope is zero, y does not change, thus is constant—a horizontal line."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {},
   "source": [
    "##### 6. In linear regression, what are the conditions for a negative slope ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "545b80dc",
   "metadata": {},
   "source": [
    "**Ans:** If the slope is negative, y decreases as x increases and the function runs downhill."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b3b649e",
   "metadata": {},
   "source": [
    "##### 7. What is multiple linear regression and how does it work ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a706f07b",
   "metadata": {},
   "source": [
    "**Ans:** Multiple linear regression is a statistical technique used to model the relationship between a dependent variable and multiple independent variables. It extends the concept of simple linear regression, which models the relationship between two variables, to include multiple predictors.\n",
    "\n",
    "In multiple linear regression, the goal is to find a linear equation that best fits the data by minimizing the difference between the observed values of the dependent variable and the values predicted by the model. The equation takes the following general form:\n",
    "\n",
    "Y = β0 + β1X1 + β2X2 + ... + βnXn + ε\n",
    "\n",
    "Where:\n",
    "\n",
    "- Y is the dependent variable (the variable being predicted)\n",
    "- X1, X2, ..., Xn are the independent variables (also known as predictors or features)\n",
    "- β0 is the intercept (the value of Y when all the independent variables are zero)\n",
    "- β1, β2, ..., βn are the coefficients (representing the effect of each independent variable on the dependent variable)\n",
    "- ε is the error term (representing the discrepancy between the observed and predicted values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54020cf8",
   "metadata": {},
   "source": [
    "##### 8. In multiple linear regression, define the number of squares due to error ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45c8a41a",
   "metadata": {},
   "source": [
    "**Ans:** The mean squared error (MSE) tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the “errors”) and squaring them. It's called the mean squared error as you're finding the average of a set of errors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9db681d1",
   "metadata": {},
   "source": [
    "##### 9. In multiple linear regression, define the number of squares due to regression ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08d8c338",
   "metadata": {},
   "source": [
    "**Ans:** In multiple linear regression, the number of squares due to regression represents the sum of squares of the differences between the predicted values of the dependent variable and the mean of the dependent variable.\n",
    "\n",
    "Mathematically, the sum of squares due to regression (SSR) is calculated as follows:\n",
    "\n",
    "SSR = Σ(y_pred - ȳ)²\n",
    "\n",
    "Where:\n",
    "\n",
    "- SSR is the sum of squares due to regression\n",
    "- y_pred represents the predicted values of the dependent variable\n",
    "- ȳ is the mean of the dependent variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b686929",
   "metadata": {},
   "source": [
    "##### 10. In a regression equation, what is multicollinearity ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6198c9b",
   "metadata": {},
   "source": [
    "**Ans:** Multicollinearity occurs when two or more independent variables are highly correlated with one another in a regression model. This means that an independent variable can be predicted from another independent variable in a regression model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e636eaf",
   "metadata": {},
   "source": [
    "##### 11. What is heteroskedasticity, and what does it mean ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40d28c29",
   "metadata": {},
   "source": [
    "**Ans:**  Heteroskedasticity refers to a pattern of non-constant variance in the errors or residuals of a regression model. In simple terms, it means that the variability of the residuals is not consistent across different levels of the independent variables.\n",
    "\n",
    "In a regression analysis, the residuals are the differences between the observed values of the dependent variable and the values predicted by the regression model. These residuals should ideally exhibit constant variance, known as homoskedasticity. However, when heteroskedasticity is present, the spread or dispersion of the residuals varies systematically as the values of the independent variables change."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64feb7de",
   "metadata": {},
   "source": [
    "##### 12. Describe the concept of ridge regression ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bece457b",
   "metadata": {},
   "source": [
    "**Ans:** Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity.By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors. It is hoped that the net effect will be to give estimates that are more reliable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bcec9c51",
   "metadata": {},
   "source": [
    "##### 13. Describe the concept of lasso regression ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9483b689",
   "metadata": {},
   "source": [
    "**Ans:** In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5830b034",
   "metadata": {},
   "source": [
    "##### 14. What is polynomial regression and how does it work ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ef64d96",
   "metadata": {},
   "source": [
    "**Ans:** Polynomial Regression is a form of Linear regression known as a special case of Multiple linear regression which estimates the relationship as an nth degree polynomial. Polynomial Regression is sensitive to outliers so the presence  of one or two outliers can also badly affect the performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed872326",
   "metadata": {},
   "source": [
    "##### 15. Describe the basis function ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b914fa8",
   "metadata": {},
   "source": [
    "**Ans:** This is a generalization of linear regression that essentially replaces each input with a function of the input. (A linear basis function model that uses the identity function is just linear regression.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9dbc03d5",
   "metadata": {},
   "source": [
    "##### 16. Describe how logistic regression works ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3605dcde",
   "metadata": {},
   "source": [
    "**Ans:** Logistic regression uses an equation as the representation, very much like linear regression. Input values (x) are combined linearly using weights or coefficient values (referred to as the Greek capital letter Beta) to predict an output value (y)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
