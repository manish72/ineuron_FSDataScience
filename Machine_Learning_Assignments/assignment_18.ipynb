{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {},
   "source": [
    "##### 1. What is the difference between supervised and unsupervised learning? Give some examples to illustrate your point ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af0b6fe4",
   "metadata": {},
   "source": [
    "**Ans:** Supervised and unsupervised learning are two fundamental approaches in machine learning that differ in their objectives, training data, and the nature of the learning process.\n",
    "\n",
    "**Supervised Learning:**\n",
    "Supervised learning involves training a model using labeled data, where each data point is associated with a corresponding target or outcome variable. The objective is to learn a mapping between the input features and the known target variable, allowing the model to make predictions on unseen data.\n",
    "\n",
    "Examples of supervised learning algorithms and applications include:\n",
    "\n",
    "- Linear Regression: Predicting housing prices based on features like area, number of rooms, and location.\n",
    "- Classification (e.g., Logistic Regression, Decision Trees): Identifying whether an email is spam or not based on its content and metadata.\n",
    "- Support Vector Machines (SVM): Distinguishing between different types of tumors based on medical imaging data.\n",
    "\n",
    "**Unsupervised Learning:**\n",
    "Unsupervised learning, on the other hand, involves training a model using unlabeled data, where there are no predefined target variables or outcomes. The objective is to discover inherent patterns, structures, or relationships within the data without prior knowledge.\n",
    "\n",
    "Examples of unsupervised learning algorithms and applications include:\n",
    "\n",
    "- Clustering (e.g., K-means, Hierarchical Clustering): Grouping similar customers based on their purchasing behavior for market segmentation.\n",
    "- Principal Component Analysis (PCA): Reducing the dimensionality of high-dimensional data while preserving most of the important information.\n",
    "- Anomaly Detection: Identifying unusual patterns or outliers in a dataset, such as fraudulent transactions in credit card data.\n",
    "In supervised learning, the model learns from labeled examples and aims to generalize the relationships between input features and known target variables to make predictions on new, unseen data. The training process involves minimizing the discrepancy between the predicted and actual target values.\n",
    "\n",
    "In unsupervised learning, the model explores the inherent structure of the data without explicit labels or targets. It identifies patterns, clusters, or dependencies within the data, which can be useful for exploratory analysis, data preprocessing, or generating insights."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {},
   "source": [
    "##### 2. Mention a few unsupervised learning applications ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e4d7be2",
   "metadata": {},
   "source": [
    "**Ans:** Few applications of unsupervised learning are\n",
    "\n",
    "- **Market Segmentation**: Unsupervised learning algorithms like clustering can be used to group customers based on their purchasing behavior, demographics, or preferences. This helps businesses understand their customer base and tailor marketing strategies for different segments.\n",
    "- **Image and Document Clustering**: Unsupervised learning algorithms can analyze large collections of images or documents and group similar ones together. This can be useful for organizing and categorizing data, as well as for recommendation systems or content discovery.\n",
    "- **Anomaly Detection**: Unsupervised learning techniques can identify unusual patterns or outliers in datasets. This is particularly valuable in fraud detection, network intrusion detection, or system health monitoring, where identifying abnormal behavior is crucial.\n",
    "- **Dimensionality Reduction**: Unsupervised learning methods like Principal Component Analysis (PCA) or t-SNE (t-Distributed Stochastic Neighbor Embedding) can reduce the dimensionality of high-dimensional datasets while preserving important patterns or relationships. This aids in data visualization, feature selection, or data compression.\n",
    "- **Recommendation Systems**: Unsupervised learning algorithms can analyze user behavior, preferences, or item characteristics to generate personalized recommendations. This is widely used in e-commerce platforms, streaming services, and content recommendation engines.\n",
    "- **Topic Modeling**: Unsupervised learning techniques such as Latent Dirichlet Allocation (LDA) can discover hidden topics or themes within a collection of documents. This is useful for organizing and summarizing large text corpora, sentiment analysis, or content analysis.\n",
    "- **Ancestry and Genetics**: Unsupervised learning algorithms can analyze genetic data to identify patterns, clusters, or relationships among individuals. This can assist in understanding genetic ancestry, population genetics, or disease susceptibility.\n",
    "- **Natural Language Processing (NLP)**: Unsupervised learning techniques like word embeddings (e.g., Word2Vec or GloVe) can capture semantic relationships between words or documents. This facilitates tasks like text classification, sentiment analysis, or machine translation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {},
   "source": [
    "##### 3. What are the three main types of clustering methods? Briefly describe the characteristics of each ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2abf909",
   "metadata": {},
   "source": [
    "**Ans:** The various types of clustering are:\n",
    "- **Connectivity-based Clustering (Hierarchical clustering):** Hierarchical Clustering is a method of unsupervised machine learning clustering where it begins with a pre-defined top to bottom hierarchy of clusters. It then proceeds to perform a decomposition of the data objects based on this hierarchy, hence obtaining the clusters. This method  follows two approaches based on the direction of progress, i.e., whether it is the top-down or bottom-up flow of creating clusters.\n",
    "          \n",
    "          \n",
    "- **Centroids-based Clustering (Partitioning methods):** Centroid based clustering is considered as one of the most simplest clustering algorithms, yet the most effective way of creating clusters and assigning data points to it. The intuition behind centroid based clustering is that a cluster is characterized and represented by a central vector and data points that are in close proximity to these vectors are assigned to the respective clusters.  \n",
    "      \n",
    "      \n",
    "- **Density-based Clustering (Model-based methods):** If one looks into the previous two methods that we discussed, one would observe that both hierarchical and centroid based algorithms are dependent on a distance (similarity/proximity)  metric. The very definition of a cluster is based on this metric. Density-based clustering methods take density into consideration  instead of distances. Clusters are considered as the densest region in a data space, which is separated by regions of lower object density and it is defined as a maximal-set of connected points.\n",
    "    \n",
    "    \n",
    "- **Distribution-based Clustering:** Until now, the clustering techniques as we know are based around either proximity (similarity/distance) or composition (density). There is a family of clustering algorithms that take a totally different metric into consideration â€“ probability. Distribution-based clustering creates and groups data points   based on their likely hood of belonging to the same probability distribution (Gaussian, Binomial etc.) in the data.\n",
    "       \n",
    "       \n",
    "- **Fuzzy Clustering:** The general idea about clustering revolves around assigning data points to mutually exclusive  clusters, meaning, a data point always resides uniquely inside a cluster and it cannot belong to more than one cluster. Fuzzy clustering methods change this paradigm by assigning a data-point to multiple clusters with a quantified degree of belongingness metric. The data-points that are in proximity to the center of a cluster, may  also belong in the cluster that is at a higher degree than points in the edge of a cluster. The possibility of which  an element belongs to a given cluster is measured by membership coefficient that vary from 0 to 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {},
   "source": [
    "##### 4. Explain how the k-means algorithm determines the consistency of clustering ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14fbf5a9",
   "metadata": {},
   "source": [
    "**Ans:** Calculate the Within-Cluster-Sum of Squared Errors (WSS) for different values of k, and choose the k for which WSS  becomes first starts to diminish. In the plot of WSS-versus-k, this is visible as an elbow. Within-Cluster-Sum of  Squared Errors sounds a bit complex."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {},
   "source": [
    "##### 5. With a simple illustration, explain the key difference between the k-means and k-medoids algorithms ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59735150",
   "metadata": {},
   "source": [
    "**Ans:** K-means attempts to minimize the total squared error, while k-medoids minimizes the sum of dissimilarities between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k -means algorithm, k -medoids chooses datapoints as centers ( medoids or exemplars)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {},
   "source": [
    "##### 6. What is a dendrogram, and how does it work? Explain how to do it ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ec88301",
   "metadata": {},
   "source": [
    "**Ans:** A dendrogram is a diagram that shows the attribute distances between each pair of sequentially merged classes After each merging, the distances between all pairs of classes are updated. The distances at which the signatures of classes are merged are used to construct a dendrogram."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b3b649e",
   "metadata": {},
   "source": [
    "##### 7. What exactly is SSE? What role does it play in the k-means algorithm ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa5e6d20",
   "metadata": {},
   "source": [
    "**Ans:** I am going to refer to it as SSE, which stands for Sum of Squared Errors. The regression line is the line made using the function we defined above. To get the SSE we calculate the distance for each of the data points from the regression line then square the it, then we add to the sum.\n",
    "\n",
    "The SSE is defined as the sum of the squared Euclidean distances of each point to its closest centroid. Since this is a measure of error, the objective of k-means is to try to minimize this value. The purpose of this figure is to show  that the initialization of the centroids is an important step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54020cf8",
   "metadata": {},
   "source": [
    "##### 8. With a step-by-step algorithm, explain the k-means procedure ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f29accb2",
   "metadata": {},
   "source": [
    "**Ans:** k-means is one of the simplest unsupervised learning algorithms that solve the well known clustering problem. The procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters) fixed apriori. The main idea is to define k centers, one for each cluster.\n",
    "\n",
    "- Step 1: Choose the number of clusters k. \n",
    "- Step 2: Select k random points from the data as centroids.\n",
    "- Step 3: Assign all the points to the closest cluster centroid. \n",
    "- Step 4: Recompute the centroids of newly formed clusters.\n",
    "- Step 5: Repeat steps 3 and 4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9db681d1",
   "metadata": {},
   "source": [
    "##### 9. In the sense of hierarchical clustering, define the terms single link and complete link ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5567b4bd",
   "metadata": {},
   "source": [
    "**Ans:** In single-link (or single linkage) hierarchical clustering, we merge in each step the two clusters whose two closest members have the smallest distance (or: the two clusters with the smallest minimum pairwise distance). Complete-link clustering can also be described using the concept of clique."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b686929",
   "metadata": {},
   "source": [
    "##### 10. How does the apriori concept aid in the reduction of measurement overhead in a business basket analysis? Give an example to demonstrate your point ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc46ef7d",
   "metadata": {},
   "source": [
    "**Ans:** Apriori algorithm assumes that any subset of a frequent itemset must be frequent. Its the algorithm behind Market Basket Analysis. So, according to the principle of Apriori, if {Grapes, Apple, Mango} is frequent, then {Grapes,Mango} must also be frequent. Here is a dataset consisting of six transactions.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
