{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ebb0324",
   "metadata": {
    "id": "4ebb0324"
   },
   "source": [
    "##### 1. In the sense of machine learning, what is a model? What is the best way to train a model?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61dfdde6",
   "metadata": {
    "id": "61dfdde6"
   },
   "source": [
    "**Ans:** In the sense of machine learning, a model is a mathematical representation of a real-world system. It is used to make predictions about the system's behavior. A model is trained on a set of data that contains examples of the system's behavior. The training process involves finding the parameters of the model that best fit the data.\n",
    "\n",
    "The best way to train a model depends on the type of model and the data that it is trained on. However, there are some general principles that can be applied to most models. These principles include:\n",
    "\n",
    "**Use a large and diverse dataset.** The more data that a model is trained on, the more accurate it will be. The data should also be diverse, so that the model can learn to generalize to new situations.\\\n",
    "**Choose the right model.** There are many different types of machine learning models, and each one is suited for a different task. For example, a decision tree is a good choice for classification tasks, while a neural network is a good choice for regression tasks.\\\n",
    "**Use a regularization technique.** Regularization is a technique that helps to prevent overfitting, which is a problem that occurs when a model learns the training data too well and is unable to generalize to new data.\\\n",
    "**Validate your model.** Once a model has been trained, it is important to validate it on a held-out dataset. This dataset should not have been used to train the model, so that it can be used to assess the model's accuracy on new data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef7fd2fb",
   "metadata": {
    "id": "ef7fd2fb"
   },
   "source": [
    "##### 2. In the sense of machine learning, explain the \"No Free Lunch\" theorem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d28fc5b",
   "metadata": {
    "id": "9d28fc5b"
   },
   "source": [
    "**Ans:** The No Free Lunch Theorem, often abbreviated as NFL or NFLT, is a theoretical finding that suggests all optimization algorithms perform equally well when their performance is averaged over all possible objective functions. In computational complexity and optimization the no free lunch theorem is a result that states that for certain types of mathematical problems, the computational cost of finding a solution, averaged over all problems in the class, is the same for any solution method.\n",
    "\n",
    "In Simple Words “No Free Lunch” theorem means we can’t rely on one model to be best of all models. We have to understand data properly and make use of ML understanding and make use of models to find best out of it. \n",
    "\n",
    "\"![0_V0GyOt3LoDVfY7y5.png](https://i0.wp.com/analyticsindiamag.com/wp-content/uploads/2019/11/no-free-lunch-theorems-analytics-india-magazine.jpg?resize=638%2C479&ssl=1)\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44b9208b",
   "metadata": {
    "id": "44b9208b"
   },
   "source": [
    "##### 3. Describe the K-fold cross-validation mechanism in detail."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a587f58e",
   "metadata": {
    "id": "a587f58e"
   },
   "source": [
    "**Ans:** In K-fold cross validation, data D is subset into k subsets randomly. Let us assume S1...Sk are the subsets where Sk is the kth randomly split subset of data D. In the first iteration, D-S1 is used for training and S1 for testing the model. When the model has been trained and tested, evaluation can be done, score is noted elsewhere and the trained model is discarded.\n",
    "\n",
    "These k-iterations go on where 1/k subset of D is always set aside for testing the data and D-1/k subsets are used for training, evaluating and discarding the model. At the end of all the iterations, average of all the evaluation scores is taken and used as output.\n",
    "\n",
    "\n",
    "\"![0_V0GyOt3LoDVfY7y5.png](https://miro.medium.com/max/1400/1*chD302ssE0O62wreunGp4A.jpeg)\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d278710",
   "metadata": {
    "id": "0d278710"
   },
   "source": [
    "##### 4. Describe the bootstrap sampling method. What is the aim of it?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ebdf40f",
   "metadata": {
    "id": "8ebdf40f"
   },
   "source": [
    "**Ans:** The bootstrap method is a statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples.\n",
    "\n",
    "Importantly, samples are constructed by drawing observations from a large data sample one at a time and returning them to the data sample after they have been chosen. This allows a given observation to be included in a given small sample more than once. This approach to sampling is called sampling with replacement. \n",
    "\n",
    "\"![0_V0GyOt3LoDVfY7y5.png](https://miro.medium.com/max/595/1*JYDm2DGzMf7nOh74nLUpAA.png)\"\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec260ac9",
   "metadata": {
    "id": "ec260ac9"
   },
   "source": [
    "##### 5. What is the significance of calculating the Kappa value for a classification model? Demonstrate how to measure the Kappa value of a classification model using a sample collection of results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80d6fe77",
   "metadata": {
    "id": "80d6fe77"
   },
   "source": [
    "**Ans:** Kappa value or Cohen's Kappa coefficient is an evaluation metric for classification models. Its significance as an evaluation metric is that it can be used to evaluate multi class classification \n",
    "models and also works on models trained on imbalanced datasets(scores like accuracy scores fail for imbalanced datasets).\n",
    "\n",
    "In simpler words It basically tells you how much better your classifier is performing over the performance of a classifier that simply  guesses at random according to the frequency of each class. Cohen's kappa is always less than or equal to 1. Values of 0 or less, indicate that the classifier is useless Cohen suggested the Kappa result be interpreted as follows: values ≤ 0 as indicating no agreement and 0.01–0.20 as none to slight, 0.21–0.40 as fair, 0.41– 0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1.00 as almost perfect agreement.\n",
    "\n",
    "\"![0_V0GyOt3LoDVfY7y5.png](https://www.statology.org/wp-content/uploads/2021/02/kappa1.png)\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e5bed08",
   "metadata": {
    "id": "9e5bed08"
   },
   "source": [
    "##### 6. Describe the model ensemble method. In machine learning, what part does it play?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1cbe3b9",
   "metadata": {
    "id": "b1cbe3b9"
   },
   "source": [
    "**Ans:** Ensemble methods or ensemble machine learning models are models where more than one models are being used spontaneously to produce better results than individually trained models. Ensemble modeling is a process where multiple diverse models are created to predict an outcome, either by using many different modeling algorithms or using different training data sets. The ensemble model then aggregates the prediction of each base model and results in once final prediction for the unseen data. \n",
    "\n",
    "The three main classes of ensemble learning methods are bagging, stacking, and boosting\n",
    "\n",
    "\"![0_V0GyOt3LoDVfY7y5.png](https://machinelearningmastery.com/wp-content/uploads/2020/11/Bagging-Ensemble.png)\"\n",
    "\n",
    "\"![0_V0GyOt3LoDVfY7y5.png](https://machinelearningmastery.com/wp-content/uploads/2020/11/Stacking-Ensemble.png)\"\n",
    "\n",
    "\"![0_V0GyOt3LoDVfY7y5.png](https://machinelearningmastery.com/wp-content/uploads/2020/11/Boosting-Ensemble.png)\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a997272",
   "metadata": {
    "id": "3a997272"
   },
   "source": [
    "##### 7. What is a descriptive model's main purpose? Give examples of real-world problems that descriptive models were used to solve."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "400eaee2",
   "metadata": {
    "id": "400eaee2"
   },
   "source": [
    "**Ans:** \n",
    "The main purpose of a descriptive model is to describe a system or entity, its properties, and how it behaves. Descriptive models are used to understand how things work, to identify patterns and trends, and to make predictions.\n",
    "\n",
    "Here are some examples of real-world problems that descriptive models were used to solve:\n",
    "\n",
    "**Weather forecasting:** Descriptive models are used to predict the weather, which is essential for planning transportation, agriculture, and other activities.\\\n",
    "**Economic forecasting:** Descriptive models are used to predict economic trends, which is helpful for businesses and governments to make decisions about investment and policy.\\\n",
    "**Medical diagnosis:** Descriptive models are used to identify patterns in medical data, which can help doctors to diagnose diseases and make treatment plans.\\\n",
    "**Risk assessment:** Descriptive models are used to assess the risk of natural disasters, accidents, and other events.\\\n",
    "**Social science research:** Descriptive models are used to study human behavior, which can help to improve our understanding of society and to develop policies that benefit society.\\\n",
    "\n",
    "Descriptive models are a powerful tool for understanding the world around us. They can be used to solve a wide variety of real-world problems, and they are constantly being developed and improved.\n",
    "\n",
    "Here are some additional examples of how descriptive models are used in the real world:\n",
    "\n",
    "**Business:** Descriptive models are used by businesses to understand their customers, their competitors, and the market. This information can be used to make decisions about pricing, marketing, and product development.\\\n",
    "**Government:** Descriptive models are used by governments to understand the economy, the population, and other factors that affect the country. This information can be used to make decisions about policy, funding, and regulation.\\\n",
    "**Education:** Descriptive models are used by educators to understand student learning, teacher effectiveness, and school performance. This information can be used to improve teaching and learning.\\\n",
    "**Healthcare:** Descriptive models are used by healthcare professionals to understand diseases, treatments, and patient outcomes. This information can be used to improve patient care.\\\n",
    "\n",
    "Descriptive models are a valuable tool for understanding the world around us. They can be used to solve a wide variety of real-world problems, and they are constantly being developed and improved.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40a8d088",
   "metadata": {
    "id": "40a8d088"
   },
   "source": [
    "##### 8. Describe how to evaluate a linear regression model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4badbea0",
   "metadata": {
    "id": "4badbea0"
   },
   "source": [
    "**Ans:** Evaluation of a linear regression model can be done using R-square. R square is calculated as the sum of squared errors in predictions made, divided by summation of all sum of squares. R square measures how much of the change in target variable can be explained by the linear regressor. Its value ranges from 0 to 1 where 0 means poor performance and 1 means good. \n",
    "Some other techniques which can be used to evaluate a linear regression model are:\n",
    "1. Mean Square Error(MSE)/Root Mean Square Error(RMSE)\n",
    "2. Mean Absolute Error(MAE)  Others are mentioned in the picture below:\n",
    "\n",
    "\"![0_V0GyOt3LoDVfY7y5.png](https://miro.medium.com/max/1400/1*G6aSSAJuMDF5RYvPeKqPzg.png)\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af4adeb3",
   "metadata": {
    "id": "af4adeb3"
   },
   "source": [
    "##### 9. Distinguish :\n",
    "1. Descriptive vs. predictive models\n",
    "2. Underfitting vs. overfitting the model\n",
    "3. Bootstrapping vs. cross-validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95de19c0",
   "metadata": {
    "id": "95de19c0"
   },
   "source": [
    "**Ans:** The differences between:\n",
    "\n",
    "**Descriptive vs. predictive models:** Descriptive models are used to summarize and describe data, while predictive models are used to make predictions about future data. Descriptive models do not try to make predictions, but they can be used to identify patterns in data that can be used to improve the accuracy of predictive models.\\\n",
    "**Underfitting vs. overfitting the model:** Underfitting occurs when a model is not able to capture the underlying relationship between the features and the target variable. This can happen if the model is too simple or if the data is not well-represented by the features. Overfitting occurs when a model is too complex and learns the training data too well. This can happen if the model has too many parameters or if the data is noisy.\\\n",
    "**Bootstrapping vs. cross-validation:** Bootstrapping is a resampling method that is used to estimate the accuracy of a model. It involves repeatedly sampling the data with replacement and training the model on each sample. The average accuracy of the models trained on the bootstrap samples is used as an estimate of the accuracy of the model on unseen data. Cross-validation is another resampling method that is used to estimate the accuracy of a model. It involves dividing the data into a training set and a test set. The model is trained on the training set and evaluated on the test set. This process is repeated multiple times, and the average accuracy of the model on the test sets is used as an estimate of the accuracy of the model on unseen data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0ea424e",
   "metadata": {
    "id": "e0ea424e"
   },
   "source": [
    "##### 10. Make quick notes on:\n",
    "1. LOOCV.\n",
    "2. F-measurement\n",
    "3. The width of the silhouette\n",
    "4. Receiver operating characteristic curve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c992609c",
   "metadata": {
    "id": "c992609c"
   },
   "source": [
    "**Ans:** The Quick notes on:\n",
    "LOOCV or Leave One Out Cross Validation is a form of K-fold cross validation where only one observation is left out for validation purpose while the rest of the data is used for model training each iteration. It is computationally taxing and should only be used for data with low dimensionality. \n",
    "\n",
    "Harmonic mean of Precision score and recall score is called F-measurement or F-score. It is formulated as 2 (pr re)/pr +re where pr is precision score and re is recall score.\n",
    "\n",
    "Estimate of average inter cluster distance to give efficacy/performance of cluster algorithms is called width of the silhouette. It can also be defined as how identical/similar a data point 'x' is to the data points inside the cluster to which x is assigned. Its value ranges from -1 to 1 where 1 means good and -1 means bad.\n",
    "\n",
    "Curve plotted between True Positive Rate and False Positive Rate is Receiver Operating Characteristics curve and is used to find the area under the curve for ROC-AUC score for binary classification evaluation. True Positive Rate and False Positive Rate are calculated for different thresholds values where thresholds take values starting from the highest probability scores assigned to data points and goes up to the lowest probability score. The curve is impacted by presence of outliers, and simple models. Extensions can be made to this curve to suit multiclass classification evaluation requirements.     "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML_Assignment_06.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
